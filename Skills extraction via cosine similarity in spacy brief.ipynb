{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(l):\n",
    "    sorted_reduced = []\n",
    "    for i in l:\n",
    "        if i not in sorted_reduced:\n",
    "                sorted_reduced.append(i)\n",
    "\n",
    "    return sorted_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skills \n",
    "final_skills = pd.read_csv(r\"C:\\Users\\Dima\\Desktop\\FeatFiles\\clusters\\final clusters.csv\")\n",
    "hardsk=[]\n",
    "\n",
    "softsk=list(final_skills[\"0\"].dropna().values)\n",
    "\n",
    "for col in final_skills.columns[1:]:  \n",
    "    hardsk+= final_skills[col].dropna().to_list()\n",
    "    \n",
    "    \n",
    "hardsk=list(map(lambda x: x.lower(), hardsk))\n",
    "hardsk = reduce(hardsk)\n",
    "\n",
    "softsk=list(map(lambda x: x.lower(), softsk))\n",
    "softsk=reduce(softsk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "Data = pd.read_csv(\"JobDataTranslated.csv\")\n",
    "#Data['Job Description'].apply(lambda x: x.lower())\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    # removing paragraph numbers\n",
    "    text = re.sub('[0-9]+.\\t','',str(text))\n",
    "    # removing new line characters\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    # removing apostrophes\n",
    "    \n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"â€” \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    # removing salutations\n",
    "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
    "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
    "    # removing any reference to outside text\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# preprocessing speeches\n",
    "Data['Cleaned Job Description'] = Data['Job Description'].apply(clean)\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    global punctuation\n",
    "    for p in punctuation:\n",
    "        s = s.replace(p, '')\n",
    "    return s\n",
    "\n",
    "Data['Cleaned Job Description'] = Data['Cleaned Job Description'].map(remove_punctuation)\n",
    "\n",
    "Data['Cleaned Job Description'] = Data['Cleaned Job Description'].apply(lambda x: x.lower())\n",
    " \n",
    "#nltk.download('stopwords')\n",
    "en_stopwords = stopwords.words('english')\n",
    "def remove_stopwords(s):\n",
    "    global en_stopwords\n",
    "    s = word_tokenize(s)\n",
    "    s = \" \".join([w for w in s if w not in en_stopwords])\n",
    "    return s\n",
    "\n",
    "# Create a new column of descriptions with no stopwords\n",
    "Data['Cleaned Job Description NoStop'] = Data['Cleaned Job Description'].map(remove_stopwords)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldie =pd.read_pickle('first_tr_data.pkl') # this is datates from 'Word Analysis' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only actual extraction part\n",
    "\n",
    "\n",
    "def get_vac_tokens(vac):\n",
    "    singles=[]\n",
    "    doubles=[]\n",
    "    vac_list=vac.split(' ')\n",
    "    for i in vac_list:\n",
    "        singles.append(nlp(i))\n",
    "        \n",
    "    for i in range(len(vac_list)-1):\n",
    "        doubles.append(nlp(vac_list[i]+' ' + vac_list[i+1]))\n",
    "    \n",
    "    return singles, doubles\n",
    "    \n",
    "\n",
    "\n",
    "#vac - vacancy text, num - index\n",
    "def get_top_skills(vac, skills,num):\n",
    "    singles, doubles = get_vac_tokens(vac)\n",
    "    \n",
    "    l_single=[]\n",
    "    l_double=[]\n",
    "    \n",
    "    two_words= [i for i in skills if i.count(' ')>0]\n",
    "    one_words=[i for i in skills if i not in two_words]\n",
    "    \n",
    "    \n",
    "    two_words=list(map(lambda x: nlp(x), two_words))\n",
    "    one_words=list(map(lambda x: nlp(x), one_words))\n",
    "   \n",
    "    for i in one_words:\n",
    "           for token2 in singles:\n",
    "                l_single.append([i.similarity(token2), i, token2.text])\n",
    "     \n",
    "    for i in two_words:\n",
    "           for token2 in doubles:\n",
    "                 l_double.append([i.similarity(token2), i, token2.text])\n",
    "                    \n",
    "                    \n",
    "    #this part is using fisrt dataset in new framework\n",
    "    z=oldie[oldie[1]==num].values[0][0]\n",
    "\n",
    "    z=list(map(lambda x: nlp(x), z))\n",
    "    \n",
    "    \n",
    "    for i in two_words:\n",
    "        for j in z:\n",
    "            l_double.append([i.similarity(j), str(i),str(j)])\n",
    "    \n",
    "    for i in one_words:\n",
    "        for j in z:\n",
    "            l_single.append([i.similarity(j), str(i),str(j)])\n",
    "                \n",
    "    return l_single,l_double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(single,double,min_max=[3,10]):\n",
    "    \n",
    "    single=[i for i in single if float(i[0])>0.5]\n",
    "    single=[[i[0],str(i[1]),i[2]] for i in single]\n",
    "    double=[i for i in double if float(i[0])>0.5]\n",
    "    double=[[i[0],str(i[1]),i[2]] for i in double]\n",
    "    \n",
    "\n",
    "    possible_clusters_single = reduce([i[1] for i in single])\n",
    "    scores = np.zeros(len(possible_clusters_single))\n",
    "    for i in single:\n",
    "        scores[possible_clusters_single.index(i[1])]+=i[0]**3+2*max(np.sign(i[0]-0.9),0)\n",
    "        \n",
    "    scores_cl = [[scores[i], possible_clusters_single[i]] for i in range(len(scores))]\n",
    "    scores_cl=sorted(scores_cl, reverse = True)\n",
    "    \n",
    "    \n",
    "    possible_clusters_double = reduce([i[1] for i in double])\n",
    "    scores = np.zeros(len(possible_clusters_double))\n",
    "    for i in double:\n",
    "        scores[possible_clusters_double.index(i[1])]+=i[0]**4+2*max(np.sign(i[0]-0.8),0)\n",
    "        \n",
    "    scores_cl +=[[scores[i], possible_clusters_double[i]] for i in range(len(scores))]\n",
    "    scores_cl =sorted(scores_cl, reverse = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    cl=[scores_cl[i][1] for i in range(min_max[0])]\n",
    "    for i in range(min_max[0],min_max[1]):\n",
    "        if scores_cl[i][0]<2:\n",
    "            break\n",
    "        cl.append(scores_cl[i][1])\n",
    "        \n",
    "    return cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_skills(num):\n",
    "    \n",
    "\n",
    "    single,double = get_top_skills(Data['Cleaned Job Description NoStop'][num], hardsk,num)\n",
    "   \n",
    "    s,d = get_top_skills(Data['Cleaned Job Description NoStop'][num], softsk,num)\n",
    "\n",
    "\n",
    "  \n",
    "    return get_clusters(single,double), get_clusters(s,d,min_max =[2,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dima\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "C:\\Users\\Dima\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:09.530714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['maintenance',\n",
       "  'engineering',\n",
       "  'management',\n",
       "  'sales operations',\n",
       "  'installation',\n",
       "  'technician',\n",
       "  'product testing',\n",
       "  'healthcare',\n",
       "  'repair',\n",
       "  'construction'],\n",
       " ['business management',\n",
       "  'critical thinking',\n",
       "  'public speaking',\n",
       "  'problem solving',\n",
       "  'logical thinking'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_skills(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dima\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "C:\\Users\\Dima\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.911202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['mechanic',\n",
       "  'installation',\n",
       "  'technician',\n",
       "  'engineering',\n",
       "  'construction',\n",
       "  'management',\n",
       "  'product testing',\n",
       "  'sales operations',\n",
       "  'maintenance'],\n",
       " ['critical thinking',\n",
       "  'business management',\n",
       "  'public speaking',\n",
       "  'leadership',\n",
       "  'logical thinking'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_skills(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dima\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "C:\\Users\\Dima\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dima\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "C:\\Users\\Dima\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "C:\\Users\\Dima\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:08:38.538078\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "super_sk_list=[]\n",
    "for i in range(len(Data)):\n",
    "    try:\n",
    "        super_sk_list.append(give_skills(i))\n",
    "    except IndexError:\n",
    "        super_sk_list.append([-1,-1])\n",
    "print(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "binary mode doesn't take an encoding argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b45a174b48aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'super_skill_list.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuper_sk_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: binary mode doesn't take an encoding argument"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('super_skill_list.pkl', 'wb') as f:\n",
    "    pickle.dump(super_sk_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
