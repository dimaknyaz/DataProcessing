{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"JobDataTranslated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess speech\n",
    "#Data['Job Description'].apply(lambda x: x.lower())\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    # removing paragraph numbers\n",
    "    text = re.sub('[0-9]+.\\t','',str(text))\n",
    "    # removing new line characters\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\",'',str(text))\n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"— \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    # removing salutations\n",
    "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
    "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
    "    # removing any reference to outside text\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# preprocessing speeches\n",
    "Data['Cleaned Job Description'] = Data['Job Description'].apply(clean)\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    global punctuation\n",
    "    for p in punctuation:\n",
    "        s = s.replace(p, '')\n",
    "    return s\n",
    "\n",
    "Data['Cleaned Job Description'] = Data['Cleaned Job Description'].map(remove_punctuation)\n",
    "\n",
    "Data['Cleaned Job Description'] = Data['Cleaned Job Description'].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN     106773\n",
       "IN      57169\n",
       "JJ      45685\n",
       "DT      40667\n",
       "NNS     32368\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data['Job Description'].apply(lambda x: x.lower())\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "corpus = \" \".join(Data['Cleaned Job Description'].tolist())\n",
    "tokens = word_tokenize(corpus)\n",
    "# Get the parts of speech tag for all words\n",
    "answer = nltk.pos_tag(tokens)\n",
    "answer_pos = [a[1] for a in answer]\n",
    "\n",
    "# print a value count for the parts of speech\n",
    "all_pos = pd.Series(answer_pos)\n",
    "all_pos.value_counts().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dima\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NN     103824\n",
       "JJ      48363\n",
       "NNS     31957\n",
       "VBP     14378\n",
       "VBG     13640\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "en_stopwords = stopwords.words('english')\n",
    "def remove_stopwords(s):\n",
    "    global en_stopwords\n",
    "    s = word_tokenize(s)\n",
    "    s = \" \".join([w for w in s if w not in en_stopwords])\n",
    "    return s\n",
    "\n",
    "# Create a new column of descriptions with no stopwords\n",
    "Data['Cleaned Job Description NoStop'] = Data['Cleaned Job Description'].map(remove_stopwords)\n",
    "\n",
    "# make a corpus of all the words in the job description\n",
    "corpus = \" \".join(Data['Cleaned Job Description NoStop'].tolist())\n",
    "\n",
    "# This is the NLTK function that breaks a string down to its tokens\n",
    "tokens = word_tokenize(corpus)\n",
    "\n",
    "answer = nltk.pos_tag(tokens)\n",
    "answer_pos = [a[1] for a in answer]\n",
    "\n",
    "all_pos = pd.Series(answer_pos)\n",
    "all_pos.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary functions from the nltk library\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# prepare corpus from the descriptions that dont have stopwords\n",
    "corpus = \" \".join(Data['Cleaned Job Description NoStop'].tolist())\n",
    "\n",
    "#tokenize words\n",
    "tokenized_corpus = nltk.word_tokenize(corpus)\n",
    "\n",
    "# lemmatize these tokens\n",
    "lemmatized_tokens = [lmtzr.lemmatize(token) for token in tokenized_corpus]\n",
    "\n",
    "# word frequencies for the lemmatized tokens\n",
    "fd = nltk.FreqDist(lemmatized_tokens)\n",
    "\n",
    "# get the top words\n",
    "top_words = []\n",
    "for key, value in fd.items():\n",
    "    top_words.append((key, value))\n",
    "\n",
    "# sort the list by the top frequencies\n",
    "top_words = sorted(top_words, key = lambda x:x[1], reverse = True)\n",
    "\n",
    "# keep top 10 words only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'service'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\Dima\\Downloads\\data_merged.csv\")\n",
    "data=pd.DataFrame(data)\n",
    "#data=data[data[\"Is requirment\"]==1]\n",
    "indicies = [i for i in range(len(data)) if data[\"Is requirment\"][i]==1]\n",
    "\n",
    "words = [data['word'][i] for i in indicies]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_word(text, word):\n",
    "    \n",
    "    indices = [i for i in range(len(text)) if text[i] == word]\n",
    "    return indices\n",
    "\n",
    "def find_words(text,words):\n",
    "    z=[]\n",
    "    for i in words:\n",
    "        if (len(find_word(text,i))!=0):\n",
    "            z.append(find_word(text,i))\n",
    "    unlist=[z[i][j] for i in range(len(z)) for j in range(len(z[i]))]\n",
    "    return unlist\n",
    "\n",
    "useless=[\"you\",\"your\",\"and\",\"of\",\"for\",\"are\",\"a\",\"to\",\"the\",\"an\",\"is\",\"also\",\"or\",\"per\",\"in\",\"we\",\"with\",\"our\",\"us\",\"this\",\"on\",\n",
    "        \"×\",\"as\",\"who\",\"on\"]\n",
    "# loc - set of indicies with all words in text \n",
    "def get_neigbour(text,loc, left=True):\n",
    "    z=[]\n",
    "    if (left):\n",
    "        for i in loc:\n",
    "            if (i!=0 and useless.count(text[i-1])==0):\n",
    "                z.append(text[i-1]+\" \"+text[i])\n",
    "    else:\n",
    "        for i in loc:\n",
    "            if (i!=len(text)-1 and useless.count(text[i+1])==0):\n",
    "                z.append(text[i]+\" \"+text[i+1])\n",
    "            \n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "req=[]\n",
    "for i in range(1000):\n",
    "    txt = Data[\"Cleaned Job Description\"][i]\n",
    "    x = txt.split()\n",
    "    new_w=get_neigbour(x,find_words(x,words),True)\n",
    "    req.append(new_w)\n",
    "\n",
    "def ugly_list_to_cute_df():\n",
    "    req_unlist = [req[i][j] for i in range(len(req)) for j in range(len(req[i]))]\n",
    "    final = list(dict.fromkeys(req_unlist))\n",
    "\n",
    "    z=[]\n",
    "    for i in final:\n",
    "        z.append(req_unlist.count(i))\n",
    "\n",
    "    datta = {'frequency':  z, 'Combo': final}\n",
    "    dtf = pd.DataFrame (datta, columns = ['frequency','Combo'])\n",
    "    dtf=dtf.sort_values(by=['frequency'], ascending=False)\n",
    "    return dtf\n",
    "\n",
    "df = ugly_list_to_cute_df()\n",
    "#df.to_csv(\"left_neighbours.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neigbours(text,loc):\n",
    "    z=[]\n",
    "    for i in loc:\n",
    "        if (i!=0 and i!=len(text)-1):\n",
    "            if (useless.count(text[i-1])==0 and useless.count(text[i+1])==0):\n",
    "                z.append(text[i-1]+\" \"+text[i]+\" \"+ text[i+1])\n",
    "            \n",
    "    return z\n",
    "\n",
    "req=[]\n",
    "for i in range(1000):\n",
    "    txt = Data[\"Cleaned Job Description\"][i]\n",
    "    x = txt.split()\n",
    "    new_w=get_neigbours(x,find_words(x,words))\n",
    "    req.append(new_w)\n",
    "    \n",
    "df = ugly_list_to_cute_df()\n",
    "df.to_csv(\"both_neigbours.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def get_synonims(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "    return set(synonyms)\n",
    "\n",
    "\n",
    "\n",
    "words_extended = set()\n",
    "for i in words:\n",
    "    words_extended=words_extended.union(get_synonims(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3691"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
